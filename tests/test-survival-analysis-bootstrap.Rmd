---
title: "Bootstrap survival analysis"
author: "Jacqueline Buros"
date: "May 20, 2016"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r init, eval = TRUE, echo = FALSE, results = 'hide'}
root_dir <- path.expand('../')
function_dir <- root_dir
stanfile_dir <- file.path(root_dir, 'stanfiles')
## note: this code assumes we are in the 'tests' directory of the repo
suppressMessages(suppressWarnings({
  library(dplyr)
  library(ggplot2)
  library(tidyr)
  library(purrr)
  library(survival)
  source(file.path(function_dir, 'simulate-data.function.R'), chdir = T)
  source(file.path(function_dir, 'prep-data.function.R'), chdir = T)
  source(file.path(function_dir, 'make-data-plots.function.R'), chdir = T)
  source(file.path(function_dir, 'simulate-survival-analysis.function.R'), chdir = T)
}))

```

## Introduction

We're going to consider the scenario in which we have simulated data according to the generative model, as if we 
didn't know how the data were generated. 

IE rather than analyze according to the generative model, we will do an association test to see which covariates of interest 
are correlated with our outcome.

## Review data simulation process

First step is to review the data-simulation process.

The simulation code are encapsulated in the **simulate_data** function: 

```{r load-function, eval = F}
source('simulate-data.function.R')
```

This function takes a lot of parameters, so that the probabilities that go into the simulation can be customized.

Here we will call it with fairly standard inputs : 

```{r sim-data, results = 'hide'}
d <- simulate_data(n = 100, max_size = 4000, max_t = 50, failure_threshold = 4, progression_threshold = 3)
```

Most of the post-processing has been scripted into two helper functions: **prep_data()** and **make_data_plots()**. 

First, we review the simulated data:

```{r plot}
#source('make-data-plots.function.R')
make_data_plots(d)
```

Then, we rescale / center covariates for analysis: 

```{r prep-data}
#source('prep-data.function.R')
res <- prep_data(d)
adata <- res$per_observation
survd <- res$per_patient
rm(res)
```

## Review standard analysis 

The hypothetical analysis we will consider here is a survival analysis, with the initial tumor diameter as the covariate. 

Here we count "failure" as the outcome of interest.

```{r standard-surv}
library(survival)
survfit <- coxph(
   formula = Surv(first_failure, failure_status) ~ rescaled_init_size
   , data = survd
   )
print(survfit)
```

Alternatively, we can consider first progression and/or failure as the outcome: 

```{r alt-surv}
survfit2 <- coxph(
  formula = Surv(first_failure_or_progression, failure_or_progression_status) ~ rescaled_init_size
  , data = survd
  )
print(survfit2)
```

Now, depending on the particular draw / random seed the simulation used, this covariate might or might not be "significantly" associated with either of the two outcomes.

How likely is this to happen by chance alone?

## Simulate 100s of draws 

To answer this, we have wrapped the simulate data -> run analysis -> inspect results process in a function, called **simulate_survival_analysis()**. By default, this function takes the same inputs as we used above. 

Let's run it 100 times to see how likely we are to pick up the importance of tumor size in patient risk.

```{r sim-survival, results = 'hide'}

#source('simulate-survival-analysis.function.R')

## run process 100x (simulate data, estimate model for association)
res <- map(seq_len(100), ~simulate_survival_analysis())

## (output suppressed)
```

... then, format results for presentation 

```{r review-results}
rdata <- 
  res %>% 
  flatten() %>% 
  map_df(~data.frame(
    outcome = .$outcome
    , estimate = .$estimate[1]
    , pval = .$pval
    , concordance = .$concordance
    )) %>%
  mutate(significance = ifelse(pval < 0.05, 'p<0.05', ifelse(pval < 0.01, 'p<0.01', 'not')))
```

## Results 

Distribution of point estimates by outcome type & by -log10(p-value)

```{r plot-est1}
ggplot(rdata, aes(y = estimate, x = outcome, colour = -1*log10(pval))) + 
  geom_jitter() + 
  facet_wrap(~outcome, scale = 'free_x', nrow = 1) + 
  scale_colour_gradient2(low = 'black', mid = 'grey', high  = 'red', midpoint = -1 * log10(0.05)) +
  scale_y_continuous('Point estimate') +
  ggtitle('Distribution of point estimates by outcome type & p-value')

```


Sample plot, separated by "significance level"

```{r plot-est2}
ggplot(rdata, aes(y = estimate, x = significance, colour = -1*log10(pval))) + 
  geom_boxplot(aes(x = significance, y = estimate, colour = NULL), outlier.size = 0) + 
  geom_jitter() + facet_wrap(~outcome, scale = 'free_x', nrow = 1) + 
  scale_colour_gradient2(low = 'black', mid = 'grey', high  = 'red', midpoint = -1 * log10(0.05)) +
  scale_y_continuous('Point estimate') +
  ggtitle('Distribution of point estimates by outcome type & significance level')

```

Summarize which percent of observations would pass the "traditional" significance test 

.. is a decent approximation to how powered we would be to detect this effect with a sample size of 100.

```{r summ-percentage}
rdata %>%
  group_by(outcome) %>%
  mutate(n_tests = n()) %>%
  group_by(outcome, significance) %>%
  summarise(n = n()
         , percent = unique(paste0(round(n / n_tests * 100,0),'%'))
         ) %>%
  ungroup() %>%
  dplyr::select(-n) %>%
  tidyr::spread( significance, percent)
```

